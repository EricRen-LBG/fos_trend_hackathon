{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BERTopic Walkthrough notebook"
      ],
      "metadata": {
        "id": "EOsq6m92LfcF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set-up"
      ],
      "metadata": {
        "id": "hPiryhCSLp6T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lzi-CIZ6ICFq"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade --quiet bertopic\n",
        "# !pip install --upgrade --quiet google-cloud-aiplatform==1.41.0\n",
        "# !pip install --upgrade --quiet langchain==0.1.6 langchain-google-vertexai==0.0.5\n",
        "# !pip install --upgrade --quiet PyPDF==4.0.1\n",
        "# !pip install --upgrade --quiet chromadb==0.4.22\n",
        "# !pip install --upgrade --quiet ragas==0.1.3\n",
        "# !pip install --upgrade --quiet tensorflow==2.15"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "import time\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "8VSm7J7YLehy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configurations"
      ],
      "metadata": {
        "id": "z28tN4K9LyMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "PROJECT_ID = \"\"\n",
        "# Get Google Cloud project ID from gcloud\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID: \", PROJECT_ID)"
      ],
      "metadata": {
        "id": "kZDxK3oyLelB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUCKET_NAME=\"playpen-basic-gcp_dv_npd-\" + PROJECT_ID + \"-bucket\"\n",
        "BUCKET_URL=\"gs://\" + BUCKET_NAME\n",
        "print(\"Bucket NAME: \", BUCKET_NAME)\n",
        "print(\"Bucket URL: \", BUCKET_URL)"
      ],
      "metadata": {
        "id": "Tgwf2J5eLemX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FILE_BLOB = \"rag/fg21-1.pdf\"    # Ref.[1]\n",
        "print(\"FILE BLOB: \", FILE_BLOB)"
      ],
      "metadata": {
        "id": "5xJ03vQbLeog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "REGION = 'europe-west2'  # London"
      ],
      "metadata": {
        "id": "5TeWWp2lLer4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SERVICE_ACCOUNT = \"playpen-5b5a22-consumer-sa@playpen-5b5a22.iam.gserviceaccount.com\"  # to be updated per project and service account"
      ],
      "metadata": {
        "id": "dDJWurdrLesj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialise Vertex AI"
      ],
      "metadata": {
        "id": "a-p5WVdfL6Xu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "vertexai.init(project=PROJECT_ID, location=REGION)"
      ],
      "metadata": {
        "id": "xKmCJEEELeup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping"
      ],
      "metadata": {
        "id": "S2ZuyVDwMEgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import regex as re\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "D77on1HiMAdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_pdf_links(entry_page_url):\n",
        "    \"\"\"Extract all pdf links from an url and return a DataFrame with title and pdf url as columns\"\"\"\n",
        "\n",
        "    response = requests.get(url=entry_page_url)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "    download_links = soup.find_all(class_=\"search-result\")\n",
        "\n",
        "    df = pd.DataFrame([\n",
        "        {\"title\": pdf_link.find(\"h4\").string, \"url\": \"https://www.financial-ombudsman.org.uk/\" + pdf_link.get(\"href\")}\n",
        "        for pdf_link in download_links\n",
        "    ])\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "MWPlaPu_MAj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_fos_url(date_from : str  = \"2024-01-01\" , date_to: str = \"2024-01-01\", industry_sector_ID: str = \"IndustrySectorID%5B1%5D=1\"):\n",
        "    \"\"\"\n",
        "    Scrapes text date from (pdf) reports from the FOS Decision website.\n",
        "    \"\"\"\n",
        "    entry_page_url = f\"https://www.financial-ombudsman.org.uk/decisions-case-studies/ombudsman-decisions/search?{industry_sector_ID}&DateFrom={date_from}&DateTo={date_to}\"\n",
        "    # Regular expression pattern to match the desired sentence\n",
        "    pattern = r\"Your search returned (\\d+) results\"\n",
        "\n",
        "    response = requests.get(url=entry_page_url)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "    # Find the matching sentence\n",
        "    matching_sentence = soup.find(string=re.compile(pattern))\n",
        "\n",
        "    # Extract the numeric value\n",
        "    if matching_sentence:\n",
        "        match = re.search(pattern, matching_sentence)\n",
        "        result_count = int(match.group(1))\n",
        "        print(f\"Found {result_count} files.\")\n",
        "    else:\n",
        "        print(\"No matching sentence found.\")\n",
        "        return None\n",
        "\n",
        "    total_results_pages = int(result_count/10)+1\n",
        "\n",
        "    # df_list =[]\n",
        "    pdf_urls_df = pd.DataFrame()\n",
        "\n",
        "    for i in tqdm(range(total_results_pages)):\n",
        "        pdf_urls = entry_page_url+f\"&Start={i*10}\"\n",
        "        pdf_urls_df = pd.concat([pdf_urls_df,get_all_pdf_links(pdf_urls)], axis=0, ignore_index=True)\n",
        "\n",
        "    return pdf_urls_df"
      ],
      "metadata": {
        "id": "uSCNGD4CMA3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_url_df = get_fos_url(date_from=\"2023-12-25\", date_to=\"2024-01-01\")\n"
      ],
      "metadata": {
        "id": "9XKGB4SOMKtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_url_df.tail()\n"
      ],
      "metadata": {
        "id": "u79FCVcOMKkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading PDF Documents"
      ],
      "metadata": {
        "id": "Kkg7ANqYMOqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema.document import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "1StMeLj6MBbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_url_exist(url):\n",
        "    \"\"\"To check the url endpoint does exist\"\"\"\n",
        "    response = requests.get(url=url)\n",
        "    return response.status_code == requests.codes.ok"
      ],
      "metadata": {
        "id": "qWQvS87lMBjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = []\n",
        "\n",
        "for index, row in tqdm(pdf_url_df.iterrows(), total=pdf_url_df.shape[0]):\n",
        "    if check_url_exist(row.url):\n",
        "\n",
        "        # -- Loading a pdf file --\n",
        "        pdf_url = row.url\n",
        "\n",
        "        loader = PyPDFLoader(pdf_url)\n",
        "        doc = loader.load()\n",
        "\n",
        "        splitter = RecursiveCharacterTextSplitter(\n",
        "                                            chunk_size=1001,\n",
        "                                            chunk_overlap=250,\n",
        "                                            separators=[\"\\n\\n\", \"\\n\", \"\\. \", \" \", \"\"]\n",
        "                                        )\n",
        "        splits = splitter.split_documents(doc)\n",
        "\n",
        "        docs.extend(splits)"
      ],
      "metadata": {
        "id": "v0JX0i4sMUWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in docs:\n",
        "    doc.metadata['file_name'] = doc.metadata['source']"
      ],
      "metadata": {
        "id": "sWQycpoWMUU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn documents into strings, ignoring the metadata\n",
        "docs_str = []\n",
        "for doc in docs:\n",
        "    doc_str = doc.dict()[\"page_content\"]\n",
        "    docs_str.append(doc_str)"
      ],
      "metadata": {
        "id": "O0F3mSCrMUTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running initial BERTopic (no tuning)"
      ],
      "metadata": {
        "id": "GJc4s5lMMZ83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "\n",
        "# Define and fit documens\n",
        "topic_model = BERTopic()\n",
        "topics, probs = topic_model.fit_transform(docs_str)"
      ],
      "metadata": {
        "id": "yCErprk4MUTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show topic information\n",
        "topic_model.get_topic_info()"
      ],
      "metadata": {
        "id": "VqT2LHLrMUPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERTopic Full Process"
      ],
      "metadata": {
        "id": "VQkv_jWaNO1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "from bertopic.representation import KeyBERTInspired\n",
        "from bertopic.vectorizers import ClassTfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN"
      ],
      "metadata": {
        "id": "MvxVQR1WMUNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence embedding\n",
        "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Dimensionality Reduction\n",
        "umap_model = UMAP(\n",
        "    n_neighbors=5,\n",
        "    n_components=5,\n",
        "    min_dist=0.05,\n",
        "    metric=\"cosine\",\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Clustering\n",
        "hdbscan_model = HDBSCAN(\n",
        "    min_cluster_size=5,\n",
        "    metric=\"euclidean\",\n",
        "    cluster_selection_method=\"eom\",\n",
        "    prediction_data=True,\n",
        ")\n",
        "\n",
        "# Tokenizer\n",
        "vectorizer_model = CountVectorizer(\n",
        "    stop_words = \"english\",\n",
        "    # ngram_range=(1,2),\n",
        ")\n",
        "\n",
        "# Topic representation\n",
        "ctfidf_model = ClassTfidfTransformer(\n",
        "    reduce_frequent_words=True,\n",
        ")\n",
        "\n",
        "# Fine-Tune Representations\n",
        "keybert_representation = {\"keybert\": KeyBERTInspired()}"
      ],
      "metadata": {
        "id": "IWT9M_E1MULj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can likely play around with different options available here. Notably the parameters in HDBSCAN and the different representations available.\n",
        "\n",
        "We could also experiment with different sentence embeddings at the start."
      ],
      "metadata": {
        "id": "kbic2AG5N0fS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = BERTopic(\n",
        "    nr_topics=\"auto\",\n",
        "    verbose=True,\n",
        "    vectorizer_model = vectorizer_model,\n",
        "    ctfidf_model = ctfidf_model,\n",
        "    umap_model = umap_model,\n",
        "    hdbscan_model = hdbscan_model,\n",
        "    min_topic_size=1,\n",
        "    representation_model = keybert_representation,\n",
        "    embedding_model = sentence_model\n",
        ")"
      ],
      "metadata": {
        "id": "slOZK7cRMUJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics, _ = bert_model.fit_transform(docs_str)"
      ],
      "metadata": {
        "id": "GYveJJIoMUHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_labels = bert_model.generate_topic_labels(\n",
        "    nr_words=5,\n",
        "    topic_prefix=True,\n",
        "    word_length=24,\n",
        "    separator=\"_\",\n",
        ")\n",
        "topic_labels"
      ],
      "metadata": {
        "id": "LGh4n0mIMUGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics_info = bert_model.get_topics()\n",
        "topics_info"
      ],
      "metadata": {
        "id": "vf5Xf_B4MUD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model.get_topic_info()"
      ],
      "metadata": {
        "id": "UOIwBF76MUCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can we use built-in LLM capability to generate the labels instead?"
      ],
      "metadata": {
        "id": "pjMF34E4Pwof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain_google_vertexai import VertexAI\n",
        "from bertopic.representation import LangChain"
      ],
      "metadata": {
        "id": "QfQT3dncMT9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_qa_chain(VertexAI(model_name='gemini-pro', temperature=0.2))"
      ],
      "metadata": {
        "id": "vpz72lLpMT7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"In three words, describe what these documents are about.\"\n",
        "representation_model = LangChain(chain, prompt=prompt)"
      ],
      "metadata": {
        "id": "mhyzijmxMT0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence embedding\n",
        "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Dimensionality Reduction\n",
        "umap_model = UMAP(\n",
        "    n_neighbors=5,\n",
        "    n_components=5,\n",
        "    min_dist=0.05,\n",
        "    metric=\"cosine\",\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Clustering\n",
        "hdbscan_model = HDBSCAN(\n",
        "    min_cluster_size=5,\n",
        "    metric=\"euclidean\",\n",
        "    cluster_selection_method=\"eom\",\n",
        "    prediction_data=True,\n",
        ")\n",
        "\n",
        "# Tokenizer\n",
        "vectorizer_model = CountVectorizer(\n",
        "    stop_words = \"english\",\n",
        "    # ngram_range=(1,2),\n",
        ")\n",
        "\n",
        "# Topic representation\n",
        "ctfidf_model = ClassTfidfTransformer(\n",
        "    reduce_frequent_words=True,\n",
        ")"
      ],
      "metadata": {
        "id": "UeS0ElcyQJZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = BERTopic(\n",
        "    nr_topics=\"auto\",\n",
        "    verbose=True,\n",
        "    vectorizer_model = vectorizer_model,\n",
        "    ctfidf_model = ctfidf_model,\n",
        "    umap_model = umap_model,\n",
        "    hdbscan_model = hdbscan_model,\n",
        "    min_topic_size=1,\n",
        "    representation_model = representation_model,\n",
        "    embedding_model = sentence_model\n",
        ")"
      ],
      "metadata": {
        "id": "95KWQ36EQTF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics, _ = bert_model.fit_transform(docs_str)"
      ],
      "metadata": {
        "id": "DqWNyrOOQUPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_labels = bert_model.generate_topic_labels(\n",
        "    nr_words=5,\n",
        "    topic_prefix=True,\n",
        "    word_length=24,\n",
        "    # separator=\"_\",\n",
        ")\n",
        "topic_labels"
      ],
      "metadata": {
        "id": "eqbR4Oi3QVGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This current set-up is not functioning properly. Can we find a good prompt to help us?"
      ],
      "metadata": {
        "id": "rE7wCRPNMBvM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Further extensions:\n",
        "\n",
        "\n",
        "*   By default each document only contains one topic. We can output the probabilities of each document belonging to a cluster. Can we generalise and improve the outputs.\n",
        "*   BERTopic has built in dynamic topic modelling. Could this be useful?\n",
        "\n"
      ],
      "metadata": {
        "id": "NgonJalhSlB2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j3DLIPPsSh1j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}